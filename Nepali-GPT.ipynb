{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models for Global Minority Languages\n",
    "## Introduction\n",
    "\n",
    "The recent success of Large Language Models (LLMs) has driven a great deal of interest in applications ranging from homework help to global domination.\n",
    "\n",
    "One of the most exciting possibilities for LLMs is their application to problems of knowledge accessibility. Much effort has been devoted to, for example, the translation of natural language into SQL, a coding language used to query large databases. \n",
    "\n",
    "LLMs can also summarize long documents which may be difficult to comprehend for individuals with learning differences. Additionally, LLMs can be used to rephrase complex or jargon-heavy writing for individuals without a large amount of expertise in a given subject matter.\n",
    "\n",
    "All these things present the possibility of a great leap forward for information accessibility. However, LLMs are almost exclusively developed in English and other majority languages. This leaves a significant gap for those who speak languages that are in the global minority.\n",
    "\n",
    "One such language is Nepali. Nepali is spoken natively by 16 million people and is used as a second language by an additional 9 million, yet it is rarely available as an option for machine translation and LLMs do not cater to its speakers.\n",
    "\n",
    "In this project I will attempt to create a first draft of solution and a potential template for similar projects that could cater to other global minority language groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generator\n",
    "\n",
    "To start, I will create a mini-GPT model for text generation uses the KerasNLP library. This model will be trained on the simplebooks-92 dataset. This dataset uses a simplified English vocabulary. This is useful both for training purposes and for creating generated output that is readily understandable by individuals who do not speak English as a first language.\n",
    "\n",
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 10:12:50.930272: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 10:12:55.394902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras_nlp\n",
    "import tensorrt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "Below I've selected some key hyperparameters. Particularly notable here is the minimum traing sequence length. This sets the smallest number of tokens that will be examined by the model during training. We want this number to be large enough that the model is not attempting to train on single words or brief phrases which may eat up training time while providing little in the way of performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 128\n",
    "MIN_TRAINING_SEQ_LEN = 450\n",
    "\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 256\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "EPOCHS = 6\n",
    "\n",
    "NUM_TOKENS_TO_GENERATE = 80\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Simplebooks Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 10:13:02.764170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.359995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.361001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.383005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.383432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.383711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.729366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.729640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.729875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-08 10:13:04.730101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9885 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "raw_train_ds = (\n",
    "    tf.data.TextLineDataset('simplebooks/simplebooks-92-raw/train.txt')\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "raw_val_ds = (\n",
    "    tf.data.TextLineDataset(\"simplebooks/simplebooks-92-raw/valid.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer for Generator\n",
    "Here I've defined the vocabulary for the model. This vocabulary is made up of words ('tokens') from the dataset that the model needs to be able to represent and understand.\n",
    "\n",
    "PAD, UNK, BOS represent padding, unknown, and beginning-of-sentence. These tokens are set aside as non-words for our purposes.\n",
    "\n",
    "I've then loaded in KerasNLP's tokenizer and used it to preprocess the data for training. This strips out punctuation, sets every word to be all lowercase and then assigns a unique integer to each word. This allows the model to train on the data as quantified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,\n",
    "    vocabulary_size = VOCAB_SIZE,\n",
    "    lowercase = True,\n",
    "    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(\"[BOS]\")\n",
    ")\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "train_ds = raw_train_ds.map(tf.autograph.experimental.do_not_convert(preprocess), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = raw_val_ds.map(tf.autograph.experimental.do_not_convert(preprocess), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)\n",
    "\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss_fn, metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_decoder (Trans  (None, None, 256)         394749    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_1 (Tra  (None, None, 256)         394749    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3387266 (12.92 MB)\n",
      "Trainable params: 3387266 (12.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 10:18:45.612434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-08 10:18:46.442102: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff733660840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-08 10:18:46.442154: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2023-08-08 10:18:46.876931: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-08 10:18:48.198173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-08-08 10:18:49.106469: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2023-08-08 10:18:49.106510: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2023-08-08 10:18:49.363897: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3169/Unknown - 258s 76ms/step - loss: 4.5749"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 10:22:55.672360: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 13188886257418549400\n",
      "2023-08-08 10:22:55.672522: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 8378862843496919279\n",
      "2023-08-08 10:22:55.672564: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 926227300504702987\n",
      "2023-08-08 10:22:55.672593: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15899955706847232733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3169/3169 [==============================] - 260s 77ms/step - loss: 4.5749 - val_loss: 4.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 10:22:57.367724: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 13329655432636820324\n",
      "2023-08-08 10:22:57.367773: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 3243849121640297520\n",
      "2023-08-08 10:22:57.367780: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 17492471859100900918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6\n",
      "3169/3169 [==============================] - 105s 32ms/step - loss: 4.0636 - val_loss: 4.0094\n",
      "Epoch 3/6\n",
      "3169/3169 [==============================] - 102s 31ms/step - loss: 3.9485 - val_loss: 3.9490\n",
      "Epoch 4/6\n",
      "3169/3169 [==============================] - 103s 31ms/step - loss: 3.8843 - val_loss: 3.9093\n",
      "Epoch 5/6\n",
      "3169/3169 [==============================] - 103s 31ms/step - loss: 3.8411 - val_loss: 3.8734\n",
      "Epoch 6/6\n",
      "3169/3169 [==============================] - 104s 31ms/step - loss: 3.8081 - val_loss: 3.8286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff7381a1b20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('text-generator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tokens = start_packer(tokenizer([\"Don't do that! Just don't!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next(prompt, cache, index):\n",
    "    logits = model(prompt)[:, index - 1, :]\n",
    "    hidden_states = None\n",
    "    return logits, hidden_states, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "[b'[BOS] \" oh , i \\' ll tell you ! \" cried mrs . snap . \" i \\' ll have to give you all your life , and you \\' ll never be able to keep the quiet of the house in the little room where you are . you have to be a thievest - - you can \\' t be on your hands . you \\' ll get it into a house - - i \\' ll do you with me . but you \\' ll be too busy to make me think of it . you see , you can \\' t get up and get back to the kitchen and keep your eye on . i \\'']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
    "output_tokens = sampler(\n",
    "    next=next,\n",
    "    prompt = prompt_tokens,\n",
    "    index=1,\n",
    ")\n",
    "txt = tokenizer.detokenize(output_tokens)\n",
    "print(f\"Generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English to Nepali Translator\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "ENG_VOCAB_SIZE = 15000\n",
    "NEP_VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = pathlib.Path('npi-eng/npi.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, nep = line.split(\"\\t\")[:2]\n",
    "    eng = eng.lower()\n",
    "    nep = nep.lower()\n",
    "    text_pairs.append((eng, nep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I've printed some example sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('he is eating.', 'उ खादै छ।')\n",
      "('my best friend is a canadian.', 'मेरो सबैभन्दा मिल्ने साथी क्यानेडियन हो।')\n",
      "(\"tom says that he's happy.\", 'टम भन्छन् कि उनी खुशी छन्।')\n",
      "(\"i don't care, but tom does.\", 'मलाई वास्ता छैन, तर टमलाई छ।')\n",
      "('it was nice knowing you.', 'तिमीलाई चिन्दा राम्रो लाग्यो।')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the sentence pairs into training, validation, and test sets. Notice that this dataset is quite small. This is one of the challenges of creating models for global minority languages. There is substantially less data to work with than if we were working with, for example, Spanish or French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574 total pairs\n",
      "1102 training pairs\n",
      "236 validation pairs\n",
      "236 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer for Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
