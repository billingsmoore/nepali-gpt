{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Tutor for Global Minority Languages\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "The recent success of Large Language Models (LLMs) has driven a great deal of interest in applications ranging from homework help to global domination.\n",
    "\n",
    "One of the most exciting possibilities for LLMs is their application to problems of knowledge accessibility.\n",
    "\n",
    "Companies like DuoLingo and Khan Academy are leveraging LLMs and other forms of machine learning to provide a replacement for a human tutor. However, there is a dearth of learning resources available for global minority languages, that is, languages that are not among the languages most widely spoken.\n",
    "\n",
    "One such language is Nepali. Nepali is spoken natively by 16 million people and is used as a second language by an additional 9 million, yet it is rarely available as an option for machine translation and LLMs do not cater to its speakers.\n",
    "\n",
    "However, before effective tools can be made for the Nepali speaking population, it must also be possible for engineers and data scientists to learn Nepali.\n",
    "\n",
    "### Project Description\n",
    "\n",
    "In this project I will endeavor to create an AI Nepali tutor. The tutor should be able to understand English language sentences from its student and provide Nepali translations of those sentences. Additionally, it should be able to generate novel sentences in English and provide Nepali translations of those sentences.\n",
    "\n",
    "Further, Nepali uses the Devanagari script: a form of writing unfamiliar to many English speakers, especially in the west. Thus, the student should be able to provide an unfamiliar sentence in Nepali, written in this script, and receive a translation from the tutor.\n",
    "\n",
    "For this project I will train a series of models. The first will translate English sentences to Nepali. The second will translate written Nepali into written English. Finally, the third will generate novel sentences in English.\n",
    "\n",
    "This project presents a unique challenge. There is very little high-quality data available for training on the Nepali language. As such, I've put significant effort into optimizing my models to be as effective as possible with with very little data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English to Nepali Translation\n",
    "\n",
    "First, I will create a model to translate English sentences into Nepali sentences. To create this model, I drew on the Keras tutorial provided here:\n",
    "\n",
    "https://keras.io/examples/nlp/neural_machine_translation_with_keras_nlp/\n",
    "\n",
    "I've adapted the model from the tutorial to translate English into Nepali, rather than Spanish, and streamlined the code for simplicity and to meet my need for computational efficiency. Additionally, I've substantially altered the model in order to more fully optimize for the much, much smaller dataset available for the Nepali language.\n",
    "\n",
    "The first step of this process is to import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.keras.activations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras_nlp\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/keras/__init__.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function \u001b[39mas\u001b[39;00m _print_function\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/keras/activations/__init__.py:10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function \u001b[39mas\u001b[39;00m _print_function\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m deserialize\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m elu\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m exponential\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.keras.activations'"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will establish the necessary constants for the model. \n",
    "\n",
    "I will use a batch size of 4 for my data. This an EXTREMELY small batch size. I'm using such a small batch size in hopes of it helping to account for the extreme smallness of my dataset. This will lead to slower convergence. However, with a larger number of epochs and a careful choice of optimization algorithms (to be discussed later) we can get away with it.\n",
    "\n",
    "I will train the model for 100 epochs. I had initially hoped to get away with fewer, however, with such small batch sizes, convergence takes longer.\n",
    "\n",
    "I also establish a size for the vocabulary that the model will use and the dimensions for the model to expect from the data. \n",
    "\n",
    "An interesting addition here is AUTOTUNE. tf.data.AUTOTUNE will automate optimization for training the model. This is extremely useful both for effectively utilizing computing resources, and for avoiding too much time lost to optimization tinkering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Exploring the Data\n",
    "\n",
    "Now I will import the dataset that this model will be trained on. This data comes from Anki, a free, open-source flashcard program that is popular with language learners. It is available from this link:\n",
    "\n",
    "https://www.manythings.org/anki/\n",
    "\n",
    "I've then split the sentence pairs into two sets and set every English letter to be lowercase to avoid any confusion in the model. This is not necessary for Nepali because the Devanagari script does not use upper and lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = pathlib.Path('datasets/npi-eng/npi.txt')\n",
    "\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, nep = line.split(\"\\t\")[:2]\n",
    "    eng = eng.lower()\n",
    "    text_pairs.append((eng, nep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes in the form of numerous sentence pairs. First the sentence is given in English, then in Nepali. Each pair also has a source attribution, but that won't be necessary for the model. Below, I've printed some representative sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tom and mary are great friends.', 'टम र मेरी महान साथी हुन्।')\n",
      "('i have a lot of back pain.', 'मलाई धेरै ढाड दुख्ने समस्या छ।')\n",
      "(\"i'm out of shape.\", 'मेरो आकार छैन।')\n",
      "(\"tom realized that i couldn't do that.\", 'टमले बुझे कि म त्यो गर्न सक्दिन।')\n",
      "('tom looked really uncomfortable.', 'टम साँच्चै असहज देखिन्थ्यो ।')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the sentence pairs into training, validation, and test sets. Notice that this dataset is quite small. This is one of the challenges of creating models for global minority languages. There is substantially less data to work with than if we were working with, for example, Spanish or French. As a result, I've allocated just 5% of the pairs to validation and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574 total pairs\n",
      "1418 training pairs\n",
      "78 validation pairs\n",
      "78 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.05 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Tokenizer\n",
    "\n",
    "The tokenizer will assign each unique word in the dataset a 'token' a unique number that allows the data to be treated numerically during model training. In order to do this, a \"vocabulary\" must first be created. This is a complete list of the unique English and Nepali words in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "\n",
    "Note that I've set aside some peculiar tokens. These correspond to whitespace,unknown characters, the beginnings and endings of sentences. I don't want the tokenizer to treat these things as words that need to be tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 22:11:39.350143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.379479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.379765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.380737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.380973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.381180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.464300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.464662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.464952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-16 22:11:39.465125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9945 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "nep_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "nep_vocab = train_word_piece(nep_samples, VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see some example words from the dataset. Note that Nepali uses a distinct writing system that may not render correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokens:  ['lot', 'when', '##ch', '##ome', 'decided']\n",
      "Nepali Tokens:  ['##्दै', 'कहाँ', 'जे', '##की', '##मी']\n"
     ]
    }
   ],
   "source": [
    "print(\"English Tokens: \", eng_vocab[150:155])\n",
    "print(\"Nepali Tokens: \", nep_vocab[200:205])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can tokenize the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")\n",
    "\n",
    "nep_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=nep_vocab, lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Next, I will preprocess each batch of data. This consists of re-assembling the English-Nepali sentence pairs. Each sentence must be padded with the \"[PAD]\" whitespace token in order to make each sequence of tokens the same length. This is because the model expects inputs of a particular shape. Once the sentence has been padded to the appropriate length, a [START] token can be appended to the beginning and an [END] token appended to the end.\n",
    "\n",
    "Finally, this assembled dataset can be split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(eng, nep):\n",
    "    batch_size = tf.shape(nep)[0]\n",
    "\n",
    "    eng = eng_tokenizer(eng)\n",
    "    nep = nep_tokenizer(nep)\n",
    "\n",
    "    # pad eng to max_sequence_length\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value = eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # add special tokens [start] and [end] and pad nep\n",
    "    nep_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length = MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value = nep_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value = nep_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value = nep_tokenizer.token_to_id(\"[PAD]\")\n",
    "    )\n",
    "\n",
    "    nep = nep_start_end_packer(nep)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "        \"encoder_inputs\": eng,\n",
    "        \"decoder_inputs\": nep[:, :-1]\n",
    "        },\n",
    "        nep[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, nep_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    nep_texts = list(nep_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, nep_texts))\n",
    "    dataset=dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "Now it's time to build the model itself. This model is an Autoencoder, which consists of an encoder and a decoder. \n",
    "\n",
    "The encoder input layer takes in a set of tokenized inputs. These inputs are then passed to a layer that accounts for the number assigned to the token as well as the position of that token in the sentence. The next layer is a typical dense Encoder layer.\n",
    "\n",
    "The decoder takes in a set of tokenized inputs from the Nepali dataset and passes them to a layer that will account for the token number and position of the token in those sentences. This is then passed to a typical dense Decoder layer.\n",
    "\n",
    "Both the Encoder and Decoder layers are helpfully provided out-of-the-box by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length = MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim = INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "eng_nep_translator = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"eng_nep_translator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"eng_nep_translator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3850240   ['encoder_inputs[0][0]']      \n",
      " ng (TokenAndPositionEmbedd                                                                       \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, None, 256)            1315072   ['token_and_position_embedding\n",
      " formerEncoder)                                                     [0][0]']                      \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, None, 15000)          9283992   ['decoder_inputs[0][0]',      \n",
      "                                                                     'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14449304 (55.12 MB)\n",
      "Trainable params: 14449304 (55.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "eng_nep_translator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "Now, I've compiled the model.\n",
    "\n",
    "Of note here is the choice of optimization algorith. I have used RMSProp. RMSProp is similar to Adagrad, which we studied in class, and as a result it converges much more quickly than, say, SGD. However, it is less susceptible to vanishing gradients. This is perfect for our small dataset with small batch sizes.\n",
    "\n",
    "The loss function is Sparse Categorical Crossentropy. Not every word appears in every sentence so the data for most natural language related tasks is necessarily sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_nep_translator.compile(\n",
    "    \"rmsprop\", \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' history = eng_nep_translator.fit(\\n    train_ds, \\n    epochs=EPOCHS, \\n    validation_data=val_ds\\n    ) '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" history = eng_nep_translator.fit(\n",
    "    train_ds, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=val_ds\n",
    "    ) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid training and retraining the model, I'll now save this model with these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_nep_translator.save('models/eng-nep-translator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for future testing, I can reopen the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_nep_translator = tf.keras.models.load_model('models/eng-nep-translator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Training Results\n",
    "\n",
    "Below, we can see how the loss and accuracy evolved over the course of training. Here we can really see how difficult it is to make effective generative tools from small datasets. Even as the model's accuracy improves substantially on the training set, the accuracy on the validation data remains unacceptably low.\n",
    "\n",
    "The loss on the validation data also never decreases, instead getting worse as time goes on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import matplotlib.pyplot as plt\\n\\nacc = history.history['accuracy']\\nval_acc = history.history['val_accuracy']\\n\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\nepochs_range = range(EPOCHS)\\n\\nplt.figure(figsize=(8, 8))\\nplt.subplot(1, 2, 1)\\nplt.plot(epochs_range, acc, label='Training Accuracy')\\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\\nplt.legend(loc='lower right')\\nplt.title('Training and Validation Accuracy')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(epochs_range, loss, label='Training Loss')\\nplt.plot(epochs_range, val_loss, label='Validation Loss')\\nplt.legend(loc='upper right')\\nplt.title('Training and Validation Loss')\\nplt.show() \""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Translated Sentences\n",
    "\n",
    "Even if the translations are perfect, the outputs of our model are not meaningful sentences. The model only outputs numerical tokens. In order to turn these into something that a human can read they need to be decoded.\n",
    "\n",
    "Below is a function to decode these translated sentences. This function takes in an English sentence, runs it through our translator model then works its way through the output of the model, converting the output into words in Nepali using our tokenizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    encoder_input_tokens = eng_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    def next(prompt, cache, index):\n",
    "        logits = eng_nep_translator([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "    \n",
    "    length = 40\n",
    "    start = tf.fill((batch_size, 1), nep_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill((batch_size, length - 1), nep_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        end_token_id=nep_tokenizer.token_to_id(\"[END]\"),\n",
    "        index=1\n",
    "    )\n",
    "    generated_sentences = nep_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Translations\n",
    "\n",
    "Now, let's look at some example translations from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 22:09:42.136154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-16 22:09:42.256265: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x10146c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-16 22:09:42.256302: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2023-08-16 22:09:42.317775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-08-16 22:09:42.332400: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2023-08-16 22:09:42.332426: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2023-08-16 22:09:42.350702: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "i don't want to fight you.\n",
      "हो मलाई्नुहुन्छ ,को म्यो छैनन ०\n",
      "\n",
      "** Example 1 **\n",
      "she is wrong.\n",
      "केण यलाई ञसको भो त्योि त्यहाँ आफ्नो ०\n",
      "\n",
      "** Example 2 **\n",
      "take one of these.\n",
      "पटक फ हामीले गर्ने यसलाई जेिलो ज ०\n",
      "\n",
      "** Example 3 **\n",
      "i hate taking notes.\n",
      "ऊए तपाईं चाहन्छु तिमी चाहन्छु तिमी मेरो ०\n",
      "\n",
      "** Example 4 **\n",
      "tom wore jeans.\n",
      "यलाई थमी तिमीया टमले गोसकोमा बाहिर चाहन्छु तिमीया राजधानी कतिेको ०\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(5):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequences(tf.constant([input_sentence]))\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Accuracy is a tricky metric for something like natural language. As such, a common metric for evaluating machine translation is the ROUGE score. The ROUGE score calculates how many n-grams (words or phrases) are shared by the model's translation and the correct translation.\n",
    "\n",
    "ROUGE-1 compares how many single words are the same. ROUGE-2 compares how many two words phrases are the same.\n",
    "\n",
    "Let's take a look at how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 22:12:23.598230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-16 22:12:23.768646: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1069cb60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-16 22:12:23.768694: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2023-08-16 22:12:23.827095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-08-16 22:12:23.842597: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2023-08-16 22:12:23.842620: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2023-08-16 22:12:23.861411: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import rouge_score\n",
    "\n",
    "rouge_1 = keras_nlp.metrics.RougeN(1)\n",
    "rouge_2 = keras_nlp.metrics.RougeN(2)\n",
    "\n",
    "for test_pair in test_pairs:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences(tf.constant([input_sentence]))\n",
    "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nepali to English Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Language Text Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
