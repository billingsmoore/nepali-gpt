{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Tutor for Global Minority Languages\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "The recent success of Large Language Models (LLMs) has driven a great deal of interest in applications ranging from homework help to global domination.\n",
    "\n",
    "One of the most exciting possibilities for LLMs is their application to problems of knowledge accessibility.\n",
    "\n",
    "Companies like DuoLingo and Khan Academy are leveraging LLMs and other forms of machine learning to provide a replacement for a human tutor. However, there is a dearth of learning resources available for global minority languages, that is, languages that are not among the languages most widely spoken.\n",
    "\n",
    "One such language is Nepali. Nepali is spoken natively by 16 million people and is used as a second language by an additional 9 million, yet it is rarely available as an option for machine translation and LLMs do not cater to its speakers.\n",
    "\n",
    "However, before effective tools can be made for the Nepali speaking population, it must also be possible for engineers and data scientists to learn Nepali.\n",
    "\n",
    "### Project Description\n",
    "\n",
    "In this project I will endeavor to create an AI Nepali tutor. The tutor should be able to understand English language sentences from its student and provide Nepali translations of those sentences. Additionally, it should be able to generate novel sentences in English and provide Nepali translations of those sentences.\n",
    "\n",
    "Further, Nepali uses the Devanagari script: a form of writing unfamiliar to many English speakers, especially in the west. Thus, the student should be able to provide an unfamiliar sentence in Nepali, written in this script, and receive a translation from the tutor.\n",
    "\n",
    "For this project I will train a series of models. The first will translate English sentences to Nepali. The second will translate written Nepali into written English. Finally, the third will generate novel sentences in English.\n",
    "\n",
    "This project presents a unique challenge. There is very little high-quality data available for training on the Nepali language. As such, I've put significant effort into optimizing my models to be as effective as possible with with very little data.\n",
    "\n",
    "A github repo for this project can be found here:\n",
    "\n",
    "https://github.com/billingsmoore/nepali-tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English to Nepali Translation\n",
    "\n",
    "First, I will create a model to translate English sentences into Nepali sentences. To create this model, I drew on the Keras tutorial provided here:\n",
    "\n",
    "https://keras.io/examples/nlp/neural_machine_translation_with_keras_nlp/\n",
    "\n",
    "I've adapted the model from the tutorial to translate English into Nepali, rather than Spanish, and streamlined the code for simplicity and to meet my need for computational efficiency. Additionally, I've substantially altered the model in order to more fully optimize for the much, much smaller dataset available for the Nepali language.\n",
    "\n",
    "The first step of this process is to import the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 10:58:55.702048: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-18 10:59:02.176579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will establish the necessary constants for the model. \n",
    "\n",
    "I will use a batch size of 4 for my data. This an EXTREMELY small batch size. I'm using such a small batch size in hopes of it helping to account for the extreme smallness of my dataset. This will lead to slower convergence. However, with a larger number of epochs and a careful choice of optimization algorithms (to be discussed later) we can get away with it.\n",
    "\n",
    "I will train the model for 100 epochs. I had initially hoped to get away with fewer, however, with such small batch sizes, convergence takes longer.\n",
    "\n",
    "I also establish a size for the vocabulary that the model will use and the dimensions for the model to expect from the data. \n",
    "\n",
    "An interesting addition here is AUTOTUNE. tf.data.AUTOTUNE will automate optimization for training the model. This is extremely useful both for effectively utilizing computing resources, and for avoiding too much time lost to optimization tinkering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Exploring the Data\n",
    "\n",
    "Now I will import the dataset that this model will be trained on. This data comes from Anki, a free, open-source flashcard program that is popular with language learners. It is available from this link:\n",
    "\n",
    "https://www.manythings.org/anki/\n",
    "\n",
    "I've then split the sentence pairs into two sets and set every English letter to be lowercase to avoid any confusion in the model. This is not necessary for Nepali because the Devanagari script does not use upper and lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = pathlib.Path('datasets/npi-eng/npi.txt')\n",
    "\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, nep = line.split(\"\\t\")[:2]\n",
    "    eng = eng.lower()\n",
    "    text_pairs.append((eng, nep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes in the form of numerous sentence pairs. First the sentence is given in English, then in Nepali. Each pair also has a source attribution, but that won't be necessary for the model. Below, I've printed some representative sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('there are always exceptions.', 'त्यहाँ सधैं अपवादहरू छन्।')\n",
      "('tom was killed in 2013.', 'टमको हत्या सन् २०१३ मा भएको थियो ।')\n",
      "('we are classmates.', 'हामी सहपाठी हौं।')\n",
      "('let tom do whatever he wants to do.', 'टमले जे गर्न चाहन्छ त्यो गर्न दिनुहोस्।')\n",
      "('tom was quicker than mary.', 'टम मेरी भन्दा छिटो थियो।')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the sentence pairs into training, validation, and test sets. Notice that this dataset is quite small. This is one of the challenges of creating models for global minority languages. There is substantially less data to work with than if we were working with, for example, Spanish or French. As a result, I've allocated just 5% of the pairs to validation and testing respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574 total pairs\n",
      "1418 training pairs\n",
      "78 validation pairs\n",
      "78 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.05 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Tokenizer\n",
    "\n",
    "The tokenizer will assign each unique word in the dataset a 'token' a unique number that allows the data to be treated numerically during model training. In order to do this, a \"vocabulary\" must first be created. This is a complete list of the unique English and Nepali words in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "\n",
    "Note that I've set aside some peculiar tokens. These correspond to whitespace,unknown characters, the beginnings and endings of sentences. I don't want the tokenizer to treat these things as words that need to be tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 10:59:12.371422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.363718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.364029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.386559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.387023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.387300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.714651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.715049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.715398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-18 10:59:14.715709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10075 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "nep_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "nep_vocab = train_word_piece(nep_samples, VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see some example words from the dataset. Note that Nepali uses a distinct writing system that may not render correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokens:  ['much', 'old', 're', 'really', '##al']\n",
      "Nepali Tokens:  ['किन', 'खान', 'तिमीलाई', 'पर्छ', 'भनेर']\n"
     ]
    }
   ],
   "source": [
    "print(\"English Tokens: \", eng_vocab[150:155])\n",
    "print(\"Nepali Tokens: \", nep_vocab[200:205])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can tokenize the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")\n",
    "\n",
    "nep_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=nep_vocab, lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Next, I will preprocess each batch of data. This consists of re-assembling the English-Nepali sentence pairs. Each sentence must be padded with the \"[PAD]\" whitespace token in order to make each sequence of tokens the same length. This is because the model expects inputs of a particular shape. Once the sentence has been padded to the appropriate length, a [START] token can be appended to the beginning and an [END] token appended to the end.\n",
    "\n",
    "Finally, this assembled dataset can be split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_nep_preprocess_batch(eng, nep):\n",
    "    batch_size = tf.shape(nep)[0]\n",
    "\n",
    "    eng = eng_tokenizer(eng)\n",
    "    nep = nep_tokenizer(nep)\n",
    "\n",
    "    # pad eng to max_sequence_length\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value = eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # add special tokens [start] and [end] and pad nep\n",
    "    nep_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length = MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value = nep_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value = nep_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value = nep_tokenizer.token_to_id(\"[PAD]\")\n",
    "    )\n",
    "\n",
    "    nep = nep_start_end_packer(nep)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "        \"encoder_inputs\": eng,\n",
    "        \"decoder_inputs\": nep[:, :-1]\n",
    "        },\n",
    "        nep[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, nep_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    nep_texts = list(nep_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, nep_texts))\n",
    "    dataset=dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(eng_nep_preprocess_batch, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "eng_nep_train_ds = make_dataset(train_pairs)\n",
    "eng_nep_val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "Now it's time to build the model itself. This model is an Autoencoder, which consists of an encoder and a decoder. \n",
    "\n",
    "The encoder input layer takes in a set of tokenized inputs. These inputs are then passed to a layer that accounts for the number assigned to the token as well as the position of that token in the sentence. The next layer is a typical dense Encoder layer.\n",
    "\n",
    "The decoder takes in a set of tokenized inputs from the Nepali dataset and passes them to a layer that will account for the token number and position of the token in those sentences. This is then passed to a typical dense Decoder layer.\n",
    "\n",
    "Both the Encoder and Decoder layers are helpfully provided out-of-the-box by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length = MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim = INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "eng_nep_translator = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"eng_nep_translator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"eng_nep_translator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3850240   ['encoder_inputs[0][0]']      \n",
      " ng (TokenAndPositionEmbedd                                                                       \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, None, 256)            1315072   ['token_and_position_embedding\n",
      " formerEncoder)                                                     [0][0]']                      \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, None, 15000)          9283992   ['decoder_inputs[0][0]',      \n",
      "                                                                     'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14449304 (55.12 MB)\n",
      "Trainable params: 14449304 (55.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "eng_nep_translator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "Now, I've compiled the model.\n",
    "\n",
    "Of note here is the choice of optimization algorith. I have used RMSProp. RMSProp is similar to Adagrad, which we studied in class, and as a result it converges much more quickly than, say, SGD. However, it is less susceptible to vanishing gradients. This is perfect for our small dataset with small batch sizes.\n",
    "\n",
    "The loss function is Sparse Categorical Crossentropy. Not every word appears in every sentence so the data for most natural language related tasks is necessarily sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_nep_translator.compile(\n",
    "    \"rmsprop\", \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' history = eng_nep_translator.fit(\\n    train_ds, \\n    epochs=EPOCHS, \\n    validation_data=val_ds\\n    ) '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_nep_history = eng_nep_translator.fit(\n",
    "    eng_nep_train_ds, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=eng_nep_val_ds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid training and retraining the model, I'll now save this model with these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_nep_translator.save('models/eng-nep-translator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for future testing, I can reopen the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_nep_translator = tf.keras.models.load_model('models/eng-nep-translator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Training Results\n",
    "\n",
    "Below, we can see how the loss and accuracy evolved over the course of training. Here we can really see how difficult it is to make effective generative tools from small datasets. Even as the model's accuracy improves substantially on the training set, the accuracy on the validation data remains unacceptably low.\n",
    "\n",
    "The loss on the validation data also never decreases, instead getting worse as time goes on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import matplotlib.pyplot as plt\\n\\nacc = history.history['accuracy']\\nval_acc = history.history['val_accuracy']\\n\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\nepochs_range = range(EPOCHS)\\n\\nplt.figure(figsize=(8, 8))\\nplt.subplot(1, 2, 1)\\nplt.plot(epochs_range, acc, label='Training Accuracy')\\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\\nplt.legend(loc='lower right')\\nplt.title('Training and Validation Accuracy')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(epochs_range, loss, label='Training Loss')\\nplt.plot(epochs_range, val_loss, label='Validation Loss')\\nplt.legend(loc='upper right')\\nplt.title('Training and Validation Loss')\\nplt.show() \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = eng_nep_history.history['accuracy']\n",
    "val_acc = eng_nep_history.history['val_accuracy']\n",
    "\n",
    "loss = eng_nep_history.history['loss']\n",
    "val_loss = eng_nep_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Translated Sentences\n",
    "\n",
    "Even if the translations are perfect, the outputs of our model are not meaningful sentences. The model only outputs numerical tokens. In order to turn these into something that a human can read they need to be decoded.\n",
    "\n",
    "Below is a function to decode these translated sentences. This function takes in an English sentence, runs it through our translator model then works its way through the output of the model, converting the output into words in Nepali using our tokenizers.\n",
    "\n",
    "Part of decoding the sequence is sampling the probabilities of tokens that should follow the existing translation. The sampler is the algorithm that is used to select that next work or token. Here I've used the Greedy sampler which simply finds the highest likelihood next word and adds it to the translated sentence. It is computationally inexpensive and because the outputs are pretty short we don't need to worry about the Greedy sampler outputting long, repetitive sentences that don't make much sense, which can be an issue with the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_nep_translate(input_sentences):\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    encoder_input_tokens = eng_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    def next(prompt, cache, index):\n",
    "        logits = eng_nep_translator([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "    \n",
    "    length = 40\n",
    "    start = tf.fill((batch_size, 1), nep_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill((batch_size, length - 1), nep_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        end_token_id=nep_tokenizer.token_to_id(\"[END]\"),\n",
    "        index=1\n",
    "    )\n",
    "    generated_sentences = nep_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Translations\n",
    "\n",
    "Now, let's look at some example translations from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 10:59:35.830764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-18 10:59:36.980078: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1051b930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-18 10:59:36.980152: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2023-08-18 10:59:38.690555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2023-08-18 10:59:40.544634: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-08-18 10:59:41.517654: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "i didn't want to go, but i did.\n",
      "ी ठ चाहन्छुौं एक अहिलेला ,ब छिन्ौं ड हुँ हुनुहुन्छान थाहाको\n",
      "\n",
      "** Example 1 **\n",
      "i know that tom is in bed.\n",
      "मैलेु दहरूे ौलाई काम थाहाको\n",
      "\n",
      "** Example 2 **\n",
      "you won't see me again.\n",
      "ह मैले ी ा अहिले टमलाई छिन्ो ्ईस थाहा , मैलेलेको\n",
      "\n",
      "** Example 3 **\n",
      "when was the last time you cried?\n",
      "गर्न0 हामी ढसदैनिएको किस हुँै ?\n",
      "\n",
      "** Example 4 **\n",
      "i'm pretty sure tom's happy in boston.\n",
      "मैले ु चाहन्छुरही ढससँगससँगिएको कि थ  एक ठदैन तपाईं दको\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(5):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = eng_nep_translate(tf.constant([input_sentence]))\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nepali to English Translation\n",
    "\n",
    "Now that the English to Nepali Translator is finished, it's time to reverse the process. \n",
    "\n",
    "There's no need to repeat the setup stages nor the importing and preprocessing of data, since it can all be reused from the first translator. We can simply refactor our preprocess_batch function to reverse which language is the input and which language is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nep_eng_preprocess_batch(eng, nep):\n",
    "    batch_size = tf.shape(nep)[0]\n",
    "\n",
    "    eng = eng_tokenizer(eng)\n",
    "    nep = nep_tokenizer(nep)\n",
    "\n",
    "    # pad eng to max_sequence_length\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value = eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # add special tokens [start] and [end] and pad nep\n",
    "    nep_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length = MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value = nep_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value = nep_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value = nep_tokenizer.token_to_id(\"[PAD]\")\n",
    "    )\n",
    "\n",
    "    nep = nep_start_end_packer(nep)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "        \"encoder_inputs\": nep,\n",
    "        \"decoder_inputs\": eng[:, :-1]\n",
    "        },\n",
    "        nep[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, nep_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    nep_texts = list(nep_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((nep_texts, eng_texts))\n",
    "    dataset=dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(nep_eng_preprocess_batch, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "nep_eng_train_ds = make_dataset(train_pairs)\n",
    "nep_eng_val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "We can reuse the same model architecture from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_eng_translator = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"nep_eng_translator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_eng_translator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "Here I've used the same loss and optimization algorithms as the other translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_eng_translator.compile(\n",
    "    \"rmsprop\", \n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_eng_history = nep_eng_translator.fit(\n",
    "    nep_eng_train_ds, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=nep_eng_val_ds\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid training and retraining the model, I'll now save this model with these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_eng_translator.save('models/nep-eng-translator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for future testing, I can reopen the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_eng_translator = tf.keras.models.load_model('models/nep-eng-translator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Training Results\n",
    "\n",
    "Below we can see a visualization of how loss and accuracy evolved over the course of training. Perhaps unsurprisingly, these results are pretty similar to the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = nep_eng_history.history['accuracy']\n",
    "val_acc = nep_eng_history.history['val_accuracy']\n",
    "\n",
    "loss = nep_eng_history.history['loss']\n",
    "val_loss = nep_eng_history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Translated Sentences\n",
    "\n",
    "Now I will repurpose our translation function from above to work in the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nep_eng_translate(input_sentences):\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    encoder_input_tokens = nep_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    def next(prompt, cache, index):\n",
    "        logits = nep_eng_translator([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "    \n",
    "    length = 40\n",
    "    start = tf.fill((batch_size, 1), eng_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill((batch_size, length - 1), eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        end_token_id=eng_tokenizer.token_to_id(\"[END]\"),\n",
    "        index=1\n",
    "    )\n",
    "    generated_sentences = eng_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Translations\n",
    "\n",
    "Let's take a look at translations going the other way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nep_texts = [pair[1] for pair in test_pairs]\n",
    "for i in range(5):\n",
    "    input_sentence = random.choice(test_nep_texts)\n",
    "    translated = nep_eng_translate(tf.constant([input_sentence]))\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Language Text Generator\n",
    "\n",
    "For the text generator, I will create a mini-GPT model for text generation uses the KerasNLP library. This model will be trained on the simplebooks-92 dataset. This dataset uses a simplified English vocabulary. This is useful both for training purposes and for creating generated output that is readily understandable by individuals who do not speak English as a first language.\n",
    "\n",
    "To create this model, I drew on the Keras suggested tutorial for text generation that can be found here:\n",
    "\n",
    "https://keras.io/examples/generative/text_generation_gpt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "Below I've selected some key hyperparameters. Particularly notable here is the minimum traing sequence length. This sets the smallest number of tokens that will be examined by the model during training. We want this number to be large enough that the model is not attempting to train on single words or brief phrases which may eat up training time while providing little in the way of performance improvements.\n",
    "\n",
    "Additionally, NUM_TOKENS_TO_GENERATE decides how many tokens will appear in the output of the model. I've set this value to be relatively low. There are two reasons for this. \n",
    "\n",
    "First, in testing, shorter outputs were more likely to be coherent sentences that expressed a single proposition. Given that outputs are meant to be translated and understood, coherence is extremely important. \n",
    "\n",
    "Second, because the output sentences are meant to be translated and learned by a student, we want them to be short and simple enough to be readily understood and learned, even by a beginner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 128\n",
    "MIN_TRAINING_SEQ_LEN = 450\n",
    "\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 256\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "NUM_CHAR_TO_GENERATE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Simplebooks Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = (\n",
    "    tf.data.TextLineDataset('datasets/simplebooks/simplebooks-92-raw/train.txt')\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(buffer_size=256)\n",
    ")\n",
    "\n",
    "raw_val_ds = (\n",
    "    tf.data.TextLineDataset(\"datasets/simplebooks/simplebooks-92-raw/valid.txt\")\n",
    "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer for Generator\n",
    "Here I've defined the vocabulary for the model. This vocabulary is made up of words ('tokens') from the dataset that the model needs to be able to represent and understand.\n",
    "\n",
    "PAD, UNK, BOS represent padding, unknown, and beginning-of-sentence. These tokens are set aside as non-words for our purposes.\n",
    "\n",
    "I've then loaded in KerasNLP's tokenizer and used it to preprocess the data for training. This strips out punctuation, sets every word to be all lowercase and then assigns a unique integer to each word. This allows the model to train on the data as quantified data.\n",
    "\n",
    "Although I've already created a vocabulary and tokenizer for the translator models above, because the set of English sentences in the translation data set is so small, it is necessary to create new ones for the generator model.\n",
    "\n",
    "Note, if you are re-running this notebook, the vocabulary computation is quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    raw_train_ds,\n",
    "    vocabulary_size = VOCAB_SIZE,\n",
    "    lowercase = True,\n",
    "    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=gen_tokenizer.token_to_id(\"[BOS]\")\n",
    ")\n",
    "\n",
    "def preprocess(inputs):\n",
    "    outputs = gen_tokenizer(inputs)\n",
    "    features = start_packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "train_ds = raw_train_ds.map(tf.autograph.experimental.do_not_convert(preprocess), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = raw_val_ds.map(tf.autograph.experimental.do_not_convert(preprocess), num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Model\n",
    "\n",
    "Now we can actually create the model. This model is another Auto-encoder.\n",
    "\n",
    "This model is very similar architecturally to the translator model. Tokens are passed to an input layer and then to an embedding layer which accounts for both the number associated with the token as well as the position of the token in its sentential context. This then downsamples the sentence to an embedding.\n",
    "\n",
    "These embeddings are then passed to a decoder layer that attempts to generate a sentence by upsampling from the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "for _ in range(NUM_LAYERS):\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "    x = decoder_layer(x)\n",
    "\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
    "generator = keras.Model(inputs=inputs, outputs=outputs)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I've compiled the model. As before, the loss function is Sparse Categorical Crossentropy. Again, natural language is almost always going to be sparse data so this is the natural choice.\n",
    "\n",
    "Here I've used Adam as the optimizer. The smallness of the dataset isn't really a concern here so I've chosen Adam for it's ability to produce higher accuracy, even though the computation costs are a little higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.compile(optimizer='adam', loss=loss_fn, metrics=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         1312768   \n",
      " ng_3 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_decoder_3 (Tra  (None, None, 256)         394749    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_4 (Tra  (None, None, 256)         394749    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, None, 5000)        1285000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3387266 (12.92 MB)\n",
      "Trainable params: 3387266 (12.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "   3168/Unknown - 126s 38ms/step - loss: 3.7765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 11:59:29.606552: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 6603943071277581536\n",
      "2023-08-18 11:59:29.606607: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 4098329605364189913\n",
      "2023-08-18 11:59:29.606640: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 16341194706631034183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3169/3169 [==============================] - 127s 38ms/step - loss: 3.7765 - val_loss: 3.8371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 11:59:30.314296: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 8472384756783010550\n",
      "2023-08-18 11:59:30.314341: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 1432970336118965353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "3169/3169 [==============================] - 121s 37ms/step - loss: 3.7560 - val_loss: 3.7982\n",
      "Epoch 3/50\n",
      "3169/3169 [==============================] - 110s 33ms/step - loss: 3.7386 - val_loss: 3.7830\n",
      "Epoch 4/50\n",
      "3169/3169 [==============================] - 107s 33ms/step - loss: 3.7217 - val_loss: 3.8098\n",
      "Epoch 5/50\n",
      "3169/3169 [==============================] - 119s 36ms/step - loss: 3.7087 - val_loss: 3.7441\n",
      "Epoch 6/50\n",
      "3169/3169 [==============================] - 110s 32ms/step - loss: 3.6967 - val_loss: 3.7557\n",
      "Epoch 7/50\n",
      "3169/3169 [==============================] - 102s 31ms/step - loss: 3.6859 - val_loss: 3.7628\n",
      "Epoch 8/50\n",
      "3169/3169 [==============================] - 101s 31ms/step - loss: 3.6766 - val_loss: 3.7362\n",
      "Epoch 9/50\n",
      "3169/3169 [==============================] - 106s 32ms/step - loss: 3.6688 - val_loss: 3.7375\n",
      "Epoch 10/50\n",
      "3169/3169 [==============================] - 108s 33ms/step - loss: 3.6617 - val_loss: 3.7417\n",
      "Epoch 11/50\n",
      "3169/3169 [==============================] - 130s 39ms/step - loss: 3.6559 - val_loss: 3.7191\n",
      "Epoch 12/50\n",
      "3169/3169 [==============================] - 114s 35ms/step - loss: 3.6492 - val_loss: 3.7226\n",
      "Epoch 13/50\n",
      "3169/3169 [==============================] - 119s 36ms/step - loss: 3.6436 - val_loss: 3.6952\n",
      "Epoch 14/50\n",
      "3169/3169 [==============================] - 116s 35ms/step - loss: 3.6389 - val_loss: 3.6960\n",
      "Epoch 15/50\n",
      "3169/3169 [==============================] - 117s 35ms/step - loss: 3.6338 - val_loss: 3.7120\n",
      "Epoch 16/50\n",
      "3169/3169 [==============================] - 111s 34ms/step - loss: 3.6303 - val_loss: 3.6950\n",
      "Epoch 17/50\n",
      "3169/3169 [==============================] - 109s 33ms/step - loss: 3.6262 - val_loss: 3.7143\n",
      "Epoch 18/50\n",
      "3169/3169 [==============================] - 109s 33ms/step - loss: 3.6224 - val_loss: 3.7224\n",
      "Epoch 19/50\n",
      "3169/3169 [==============================] - 112s 34ms/step - loss: 3.6198 - val_loss: 3.7073\n",
      "Epoch 20/50\n",
      "3169/3169 [==============================] - 113s 34ms/step - loss: 3.6160 - val_loss: 3.7016\n",
      "Epoch 21/50\n",
      "3169/3169 [==============================] - 130s 40ms/step - loss: 3.6131 - val_loss: 3.6911\n",
      "Epoch 22/50\n",
      "3169/3169 [==============================] - 121s 37ms/step - loss: 3.6101 - val_loss: 3.7070\n",
      "Epoch 23/50\n",
      "3169/3169 [==============================] - 140s 41ms/step - loss: 3.6079 - val_loss: 3.6862\n",
      "Epoch 24/50\n",
      "3169/3169 [==============================] - 133s 40ms/step - loss: 3.6052 - val_loss: 3.6918\n",
      "Epoch 25/50\n",
      "3169/3169 [==============================] - 132s 40ms/step - loss: 3.6027 - val_loss: 3.6840\n",
      "Epoch 26/50\n",
      "3169/3169 [==============================] - 124s 38ms/step - loss: 3.6007 - val_loss: 3.6916\n",
      "Epoch 27/50\n",
      "3169/3169 [==============================] - 135s 40ms/step - loss: 3.5995 - val_loss: 3.6916\n",
      "Epoch 28/50\n",
      "3169/3169 [==============================] - 121s 36ms/step - loss: 3.5968 - val_loss: 3.7013\n",
      "Epoch 29/50\n",
      "1416/3169 [============>.................] - ETA: 1:02 - loss: 3.6025"
     ]
    }
   ],
   "source": [
    "generator.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, I'll save and reload the model for future testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('models/text-generator.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = tf.keras.models.load_model('models/text-generator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Text Generator\n",
    "\n",
    "Let's see what sort of results we're getting from the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for the translator models, we need to select a sampler. The sampler is the algorithm used to pick the next work in our output sequence based on the probabilities that were calculated by the model. \n",
    "\n",
    "There are a a number of samplers to choose from. I've found the best results, the most coherent sentences, from using the Top P Sampler. \n",
    "\n",
    "The Top P Sampler takes a probability, say .9, and selects the most likely sequence of words that sum to that probability. This is different from other samplers which only select the most probable next single word. By using Top P we can get outputs that more closely resemble coherent phrases rather than strings of words that only make sense in context of the couple of words surrounding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I've defined a \"generate\" function which takes in a prompt and returns a sentence generated by our model. Note that the generated outputs need to be cleaned before they are presentable. Like with the translations they are stripped of tokens that simply convey the beginnings and endings of sentences as well as unnecessary whitespace and potentially confusing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    \n",
    "    def next(prompt, cache, index):\n",
    "        logits = generator(prompt)[:, index - 1, :]\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "    \n",
    "    def clean(txt):\n",
    "        clean_txt = txt.numpy()[0].decode(\"utf-8\")\n",
    "        clean_txt = (\n",
    "        clean_txt.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .replace(\"[BOS]\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"!\", \".\")\n",
    "        .replace(\"?\", \".\")\n",
    "        .replace(\"\\'\", \"\")\n",
    "        .replace(\"\\\"\", \"\")\n",
    "        .replace(\";\", \"\")\n",
    "        .replace(\":\", \"\")\n",
    "        )\n",
    "        clean_txt = clean_txt[:NUM_CHAR_TO_GENERATE].split(\".\")[0].strip()\n",
    "\n",
    "        return clean_txt\n",
    "\n",
    "    prompt = start_packer(gen_tokenizer(['This is a prompt']))\n",
    "    sampler = keras_nlp.samplers.TopPSampler(p=0.9)\n",
    "    output_tokens = sampler(\n",
    "        next=next,\n",
    "        prompt = prompt,\n",
    "        index=1,\n",
    "    )\n",
    "    txt = clean(gen_tokenizer.detokenize(output_tokens))\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's pass a prompt to the model and see what we get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "jo chuck was good in at the scholars that morning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt = generate()\n",
    "\n",
    "print(f\"Generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
